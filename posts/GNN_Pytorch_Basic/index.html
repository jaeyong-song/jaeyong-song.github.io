<!DOCTYPE html><html lang="ko-KR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="GNN PyTorch Code Basic (GCN, GINConv, GAE)" /><meta name="author" content="Jaeyong Song" /><meta property="og:locale" content="ko_KR" /><meta name="description" content="데이터사이언스랩 2021년 2월 23일 (화) 심화세션 “GNN 실습 및 pytorch 모델링”을 위해서 작성한 게시물입니다. GNN 관련 여러 게시물과 강의를 참고하여 만들었습니다." /><meta property="og:description" content="데이터사이언스랩 2021년 2월 23일 (화) 심화세션 “GNN 실습 및 pytorch 모델링”을 위해서 작성한 게시물입니다. GNN 관련 여러 게시물과 강의를 참고하여 만들었습니다." /><link rel="canonical" href="https://jaeyong-song.github.io/posts/GNN_Pytorch_Basic/" /><meta property="og:url" content="https://jaeyong-song.github.io/posts/GNN_Pytorch_Basic/" /><meta property="og:site_name" content="Jaeyong" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-02-12T03:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="GNN PyTorch Code Basic (GCN, GINConv, GAE)" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Jaeyong Song" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"dateModified":"2021-02-16T22:53:50+09:00","datePublished":"2021-02-12T03:00:00+09:00","description":"데이터사이언스랩 2021년 2월 23일 (화) 심화세션 “GNN 실습 및 pytorch 모델링”을 위해서 작성한 게시물입니다. GNN 관련 여러 게시물과 강의를 참고하여 만들었습니다.","mainEntityOfPage":{"@type":"WebPage","@id":"https://jaeyong-song.github.io/posts/GNN_Pytorch_Basic/"},"url":"https://jaeyong-song.github.io/posts/GNN_Pytorch_Basic/","@type":"BlogPosting","author":{"@type":"Person","name":"Jaeyong Song"},"headline":"GNN PyTorch Code Basic (GCN, GINConv, GAE)","@context":"https://schema.org"}</script><title>GNN PyTorch Code Basic (GCN, GINConv, GAE) | Jaeyong</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://cdn.jsdelivr.net/gh/cotes2020/chirpy-images/commons/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Jaeyong</a></div><div class="site-subtitle font-italic">Yonsei Univ. Applied Stat/CS student</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/jaeyong-song" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['seonbinara','yonsei.ac.kr'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>GNN PyTorch Code Basic (GCN, GINConv, GAE)</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>GNN PyTorch Code Basic (GCN, GINConv, GAE)</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Feb 12, 2021, 3:00 AM +0900" > Feb 12 <i class="unloaded">2021-02-12T03:00:00+09:00</i> </span> by <span class="author"> Jaeyong Song </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Tue, Feb 16, 2021, 10:53 PM +0900" > Feb 16 <i class="unloaded">2021-02-16T22:53:50+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5986 words">33 min</span></div></div><div class="post-content"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://pytorch-geometric.readthedocs.io/en/latest/_static/pyg_logo_text.svg" class="preview-img" alt="Preview Image"><p>데이터사이언스랩 2021년 2월 23일 (화) 심화세션 “GNN 실습 및 pytorch 모델링”을 위해서 작성한 게시물입니다. GNN 관련 여러 게시물과 강의를 참고하여 만들었습니다.</p><p>이번 게시물에서는 <code class="language-plaintext highlighter-rouge">pytorch_geometric</code>을 이용하여 GNN 모델을 구현해보고 이를 바탕으로 지도학습과 비지도학습을 진행하는 실습을 소개하겠습니다.</p><p>Standford CS224w(Fall 2019)에서 실습을 참고하였습니다.<sup id="fnref:CS224w" role="doc-noteref"><a href="#fn:CS224w" class="footnote">1</a></sup></p><p>최근 열린 강좌(Winter 2021)에서의 실습은 CS에서 다루는 구현적 내용이 많아서, 필요하신 분은 참고하시면 될 것 같습니다.<br /> (CS 전공자 분들은 해당 강좌 Colab3 과제를 해보시는 것도 좋을 것 같습니다… 보다 최신 라이브러리를 활용해서 더 효율적인 그래프 처리를 진행합니다.)<br /> 이 추가적인 모델 구현은 다음 세션에서 Pytorch Modeling을 구체적으로 실습하면서 ResNet과 함께 구현해보겠습니다.</p><p>실습에서는 PyTorch의 사용법을 간단하게 살펴보고, PyG(Pytorch Geometric)을 이용해서 GNN 실습을 진행해보고, 텐서보드에서 그 진행상황을 기록하고, Unsupervised의 임베딩을 matplotlib으로 시각화 해보겠습니다.</p><p>참고) GNN 모델링 시에 PyTorch Geometric Documentation이 매우 자세한 편이라서 이를 참고하는 것이 베스트입니다.<sup id="fnref:PyG_Doc" role="doc-noteref"><a href="#fn:PyG_Doc" class="footnote">2</a></sup></p><h2 id="0단계-pytorch-준비-및-복습">0단계: PyTorch 준비 및 복습</h2><p>파이토치와 그외 사이킷런을 이용한 평가 등을 진행하기 위해 관련 라이브러리를 임포트하겠습니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="n">metrics</span>
</pre></table></code></div></div><p>이전에 CNN 세션에서 파이토치를 사용하는 방법을 익혔지만 다시한번 복습을 진행하겠습니다. 여기서는 Dataloader의 사용에 집중하시면 됩니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1">## transformations
</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()])</span>

<span class="c1">## download and load training dataset
</span><span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                        <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="c1"># 데이터로더를 활용하면 사이즈별로 나눠주고 iterator 형식으로 저장하며, 뒤에서 보겠지만, (data, label) 와 같은 형식으로 건네줍니다.
</span><span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1">## download and load testing dataset
</span><span class="n">testset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                       <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                         <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="c1"># MNIST의 6만개 데이터셋과 일치합니다
</span><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trainset</span><span class="p">))</span>
<span class="c1"># 28x28을 출력하고 싶다면 아래를 실행해보시면 됩니다.
# print(trainset[10])
</span></pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>60000
</pre></table></code></div></div><p>파이토치의 모델은 <code class="language-plaintext highlighter-rouge">nn.Module</code>을 슈퍼클래스로 가집니다.<br /> 그외 아래에서 보이는 레이어들에 대해서는 이전 CNN 세션에서 다뤘기 때문에 패스하겠습니다.<br /> 파이토치 모델에서 중요한 점은 <code class="language-plaintext highlighter-rouge">forward</code> 메소드를 반드시 구현해줘야한다는 것입니다. 이를 통해 parameter 업데이트와 미분이 가능하기 때문입니다. 복잡한 것이 아니라 단계적으로 layer에 값을 통과시켜주도록 메소드를 구현해주면 됩니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># 28x28x1 =&gt; 26x26x32
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">26</span> <span class="o">*</span> <span class="mi">26</span> <span class="o">*</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 32x1x28x28 =&gt; 32x32x26x26
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># flatten =&gt; 32 x (32*26*26)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1">#x = x.view(32, -1)
</span>
        <span class="c1"># 32 x (32*26*26) =&gt; 32x128
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">d1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># logits =&gt; 32x10
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">d2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></table></code></div></div><p>아래에서는 간단하게 파이토치의 <code class="language-plaintext highlighter-rouge">device</code> 개념에 대해서 살펴보도록 하겠습니다.<br /> Nvidia cuda가 제대로 설정이 되어있다면, <code class="language-plaintext highlighter-rouge">torch.cuda.is_available()</code>에서 True가 나오고 아니라면 False가 나오게 됩니다.<br /> cuda 코어를 활용하면 병렬 연산을 빠르게 수행할 수 있기 때문에 더 빠른 계산을 진행할 수 있습니다.<br /> (아마 이번 실습 수준에서는 gpu가 필요없겠지만, 실습 수준일 조금만 벗어나도 반드시 필요합니다. 아마 여러분이 진행하시는 모델링 프로젝트에서도 반드시 필요할 것이라고 생각됩니다. 그러한 경우에는 구글 코랩을 활용하시거나 개인 gpu를 활용해서 진행하시길 바랍니다.)</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s">'cuda:0'</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s">'cpu'</span>
    
<span class="k">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>cpu
</pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">.to()</code> 메소드를 이용하면 객체를 gpu로 송신할 수 있고 그 이후에 cuda를 이용해서 연산이 가능합니다.<br /> 연산을 진행할 때 cuda 위의 객체(in graphic memory)와 cpu 위의 객체(in memory)를 같이 연산할 경우 오류가 나기 때문에 유의하셔야합니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

<span class="n">ta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">tb</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">ta</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ta</span> <span class="o">@</span> <span class="n">tb</span><span class="p">)</span> <span class="c1"># matmul 함수와 동일합니다.
</span></pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>tensor([[1., 2.],
        [3., 4.]], dtype=torch.float64)
tensor([[3., 3.],
        [7., 7.]], dtype=torch.float64)
</pre></table></code></div></div><p>이전에 공부했듯이, 모델을 설정하고 해당 모델을 gpu 상에 올려줍니다. <br /> 또한 <code class="language-plaintext highlighter-rouge">criterion(loss)</code>함수를 설정해주고, GD를 위한 <code class="language-plaintext highlighter-rouge">optimizer</code>를 설정해줍니다.<br /> 모델과 <code class="language-plaintext highlighter-rouge">criterion</code>, <code class="language-plaintext highlighter-rouge">optimizer</code>간의 상호작용은 뒤에서 설명하겠습니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># 실습이니 1 에폭만 돌립니다... 한 5에폭 돌리면 98-99퍼 accuracy 나옵니다
</span>
<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s">'cuda:0'</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s">'cpu'</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></table></code></div></div><p>Now let’s write our training loop. For each minibatch (accessed by enumerating through our data loader <code class="language-plaintext highlighter-rouge">trainloader</code>), we run our data through <code class="language-plaintext highlighter-rouge">model</code> in a forward pass, then compute the loss with <code class="language-plaintext highlighter-rouge">criterion</code>. We call <code class="language-plaintext highlighter-rouge">optimizer.zero_grad()</code> to zero out the gradients from the previous round of training, followed by <code class="language-plaintext highlighter-rouge">loss.backward()</code> to backpropagate the new round of gradients and finally <code class="language-plaintext highlighter-rouge">optimizer.step()</code> to adjust the model parameters based on these gradients.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">train_running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1">## training step
</span>    <span class="c1"># 앞에서 설명한 바와 마찬가지로 (data, label) 형태로 배치사이즈에 맞게 반환해줍니다.
</span>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
        
        <span class="c1"># in memory의 데이터를 gpu로 올려야 연산이 가능합니다.
</span>        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1">## forward + backprop + loss
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span> <span class="c1"># 모델에 데이터를 통과시켜서 계산합니다
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="c1"># 계산한 logit(CELoss 입니다...)을 loss함수에 넣어서 계산합니다.
</span>        <span class="c1"># 이부분에서 왜 loss를 이용했는데 optimizer.step()을 하면 업데이트 되는지 궁금해 하시는 분들이 많습니다.
</span>        <span class="c1"># 파이토치 코드를 보고 모델을 실제로 디버깅 해보시면, (디버깅 모드에서 model을 print해서 보시면 됩니다.)
</span>        <span class="c1"># loss는 파이토치 모델 내의 gradient 변수에 값을 저장합니다.
</span>        <span class="c1"># 그런데, optimizer는 model.parameters()를 통해서 모델과 연결되어있습니다.
</span>        <span class="c1"># 따라서 gradient update와 parameter update가 가능합니다.
</span>        <span class="c1"># (CS224w 강의에서 애매하게 답변이 나왔는데, 정확한 분석은 위와 같습니다.)
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># 중요한 부분입니다: gradient 초기화를 해주지 않으면 이전 gradient와 accum 합니다. (순서가 중요)
</span>        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 계산한 loss 함수를 이용해서 미분값을 계산합니다
</span>
        <span class="c1">## update model params
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># 계산한 미분값을 이용해서 파라미터를 업데이트 합니다.
</span>
        <span class="n">train_running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
        <span class="n">train_acc</span> <span class="o">+=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">flatten</span><span class="p">()</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">'Epoch: %d | Loss: %.4f | Train Accuracy: %.2f'</span> \
          <span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">train_running_loss</span> <span class="o">/</span> <span class="n">i</span><span class="p">,</span> <span class="n">train_acc</span><span class="o">/</span><span class="n">i</span><span class="p">))</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Epoch: 0 | Loss: 1.6130 | Train Accuracy: 0.85
</pre></table></code></div></div><p>테스트를 위해서는 forward만 진행하면 됩니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">test_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">testloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">+=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">flatten</span><span class="p">()</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">flatten</span><span class="p">().</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span>
        
<span class="k">print</span><span class="p">(</span><span class="s">'Test Accuracy: %.2f'</span><span class="o">%</span><span class="p">(</span><span class="n">test_acc</span><span class="o">/</span><span class="n">i</span><span class="p">))</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Test Accuracy: 0.97
</pre></table></code></div></div><h2 id="1단계-pytorch-geometric-등-setup">1단계: PyTorch Geometric 등 Setup</h2><p>GNN 생성을 위한 PyG와 텐서보드X를 설치합니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="c1"># !pip install --verbose --no-cache-dir torch-scatter
# !pip install --verbose --no-cache-dir torch-sparse
# !pip install --verbose --no-cache-dir torch-cluster
# !pip install torch-geometric
# !pip install tensorboardX
</span></pre></table></code></div></div><p>필요한 라이브러리들을 임포트합니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">import</span> <span class="nn">torch_geometric.nn</span> <span class="k">as</span> <span class="n">pyg_nn</span>
<span class="kn">import</span> <span class="nn">torch_geometric.utils</span> <span class="k">as</span> <span class="n">pyg_utils</span>

<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="n">nx</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># 실습용 그래프 데이터와 이를 사용하기 위한 데이터로더입니다.
</span><span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">TUDataset</span>
<span class="kn">from</span> <span class="nn">torch_geometric.datasets</span> <span class="kn">import</span> <span class="n">Planetoid</span>
<span class="kn">from</span> <span class="nn">torch_geometric.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">torch_geometric.transforms</span> <span class="k">as</span> <span class="n">T</span>

<span class="c1"># 텐서보드를 이용해서 시각화하기 위해 필요합니다.
</span><span class="kn">from</span> <span class="nn">tensorboardX</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="c1"># 임베딩의 시각화를 위해서 필요합니다.
</span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</pre></table></code></div></div><h2 id="2단계-gnn-모델-구성">2단계: GNN 모델 구성</h2><p>GNN을 이용한 모델링에는 3가지가 있습니다. (1: 노드 분류, 2: 그래프 분류, 3: 링크 예측)<br /> 우선, 노드분류와 그래프분류 모델링을 살펴보고, 뒤의 unsupervised에서 링크 예측을 진행해보겠습니다. <br /> <code class="language-plaintext highlighter-rouge">build_conv_model</code> 메소드에서는 어떤 노드분류와 그래프분류 모델링을 위한 각 모델을 골라서 레이어를 만들어줍니다. <br /> PyTorch Geometric에서는 이들 레이어에 대해서 바로 사용가능한 모듈을 제공해줍니다. Winter 2021 강좌에서는 이러한 레이어를 GraphSAGE와 GAT에 대해 구현해보는 것을 과제로 하고 있습니다. 사실 이 모델은 https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/sage_conv.html#SAGEConv, https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/gcn_conv.html#GCNConv 이곳을 살펴보면 되며, 앞의 세션에서 살펴본 내용을 그대로 torch 연산을 통해 진행하는 것 외에 특별한 점은 없습니다.<br /> 아래에서는 3레이어의 conv 레이어를 이용하고 마지막으로 mean pooling과 fc 레이어 2개를 이용해서 진행하였습니다. <br /> 분류를 목적으로 하기 때문에 loss의 경우에는 nll loss를 이용하였습니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">GNNStack</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s">'node'</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GNNStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">task</span> <span class="o">=</span> <span class="n">task</span>
        <span class="c1"># 모듈리스트를 이용해서 레이어 추가
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">convs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">build_conv_model</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
        <span class="c1"># 정규화 레이어 추가
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">lns</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">))</span>
        <span class="c1"># 총 3개 레이어 이므로 2개 추가
</span>        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">build_conv_model</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>

        <span class="c1"># post-message-passing
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">post_mp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">),</span> 
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">task</span> <span class="o">==</span> <span class="s">'node'</span> <span class="ow">or</span> <span class="bp">self</span><span class="p">.</span><span class="n">task</span> <span class="o">==</span> <span class="s">'graph'</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">RuntimeError</span><span class="p">(</span><span class="s">'Unknown task.'</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.25</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="k">def</span> <span class="nf">build_conv_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="c1"># refer to pytorch geometric nn module for different implementation of GNNs.
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">task</span> <span class="o">==</span> <span class="s">'node'</span><span class="p">:</span>
            <span class="c1"># 원래는 GCNConv를 이용하지만, 아래에서 구현해본 결과를 살펴보기 위해서 해볼 것!
</span>            <span class="k">return</span> <span class="n">pyg_nn</span><span class="p">.</span><span class="n">GCNConv</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
            <span class="c1">#return CustomConv(input_dim, hidden_dim)
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pyg_nn</span><span class="p">.</span><span class="n">GINConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
                                  <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">batch</span>
        <span class="k">if</span> <span class="n">data</span><span class="p">.</span><span class="n">num_node_features</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">num_nodes</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 각 레이어 사이에는 ReLU와 오버피팅을 막기 위한 드롭아웃 추가
</span>        <span class="c1"># 그러나 마지막 레이어에는 추가하지 말아야함... 성능상 이유
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">convs</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">x</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lns</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># 그래프 분류의 경우에는 mean pool 진행해야함
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">task</span> <span class="o">==</span> <span class="s">'graph'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="p">.</span><span class="n">global_mean_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        
        <span class="c1"># 분류를 위한 fc 통과
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post_mp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">emb</span><span class="p">,</span> <span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></table></code></div></div><p>Here pyg_nn.GCNConv and pyg_nn.GINConv are instances of MessagePassing.<br /> (모든 PyG에서의 모델은 그래프의 기본인 MessagePassig을 슈퍼 클래스로 가진다.)<br /> They define a single layer of graph convolution, which can be decomposed into:<br /> (이들은 아래와 같은 이전 세션에서 다룬 기본 연산을 진행한다.)</p><ul><li>Message computation<li>Aggregation<li>Update<li>Pooling</ul><p>Here we give an example of how to subclass the pytorch geometric MessagePassing class to derive a new model (rather than using existing GCNConv and GINConv).<br /> (아래 예시에서는 실제로 이러한 사실을 감안해서 GCNConv를 구현해본다.)</p><p>아래와 같은 특성을 이용해서 실제로 GCNConv를 구현해보고, 이를 위의 모델과 만든 모델 간 각각 실행해서 효과가 동일한지 살펴볼 것! We make use of <code class="language-plaintext highlighter-rouge">MessagePassing</code>’s key building blocks:</p><ul><li><code class="language-plaintext highlighter-rouge">aggr='add'</code>: The aggregation method to use (“add”, “mean” or “max”).<li><code class="language-plaintext highlighter-rouge">propagate()</code>: The initial call to start propagating messages. Takes in the edge indices and any other data to pass along (e.g. to update node embeddings).<li><code class="language-plaintext highlighter-rouge">message()</code>: Constructs messages to node i. Takes any argument which was initially passed to propagate().<li><code class="language-plaintext highlighter-rouge">update()</code>: Updates node embeddings. Takes in the output of aggregation as first argument and any argument which was initially passed to propagate().</ul><p>여기를 참고하면 큰 도움이 됩니다.<br /> https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html#implementing-the-gcn-layer</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">CustomConv</span><span class="p">(</span><span class="n">pyg_nn</span><span class="p">.</span><span class="n">MessagePassing</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CustomConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">aggr</span><span class="o">=</span><span class="s">'add'</span><span class="p">)</span>  <span class="c1"># "Add" aggregation.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        
<span class="c1">#         # 방법2를 해보기 위해서 추가함.
#         self.lin_self = nn.Linear(in_channels, out_channels)
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
        <span class="c1"># x has shape [N, in_channels]
</span>        <span class="c1"># edge_index has shape [2, E]
</span>        
        <span class="c1"># 방법 1 : self loop를 메소드로 추가
</span>        <span class="c1"># Step 1: Add self-loops to the adjacency matrix.
</span>        <span class="n">edge_index</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pyg_utils</span><span class="p">.</span><span class="n">add_self_loops</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="c1"># Step 2: Linearly transform node feature matrix.
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
        
<span class="c1">#         # 방법 2: 수동으로 Identity Matrix 더해줌
#         # Step 1: Remove self-loops from the adjacency matrix. (add later)
#         edge_index, _ = pyg_utils.remove_self_loops(edge_index)
#         # Step 2: Linearly transform node feature matrix.
#         self_x = self.lin_self(x)
#         return self_x + self.propagate(edge_index, size=(x.size(0), x.size(0)), x=self.lin(x))
</span>
    <span class="k">def</span> <span class="nf">message</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_j</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="c1"># Compute messages
</span>        <span class="c1"># x_j has shape [E, out_channels]
</span>
        <span class="c1"># Step 3: Compute normalization.
</span>        <span class="c1"># Step 4: Normalize node features.
</span>        <span class="c1"># Step 4-5: Start propagating messages.
</span>        <span class="n">row</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="n">edge_index</span>
        <span class="n">deg</span> <span class="o">=</span> <span class="n">pyg_utils</span><span class="p">.</span><span class="n">degree</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x_j</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">deg_inv_sqrt</span> <span class="o">=</span> <span class="n">deg</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="n">deg_inv_sqrt</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">*</span> <span class="n">deg_inv_sqrt</span><span class="p">[</span><span class="n">col</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">norm</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_j</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">aggr_out</span><span class="p">):</span>
        <span class="c1"># aggr_out has shape [N, out_channels]
</span>        
        <span class="k">return</span> <span class="n">aggr_out</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">GNNCustomStack</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s">'node'</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GNNCustomStack</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">task</span> <span class="o">=</span> <span class="n">task</span>
        <span class="c1"># 모듈리스트를 이용해서 레이어 추가
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">convs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">build_conv_model</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
        <span class="c1"># 정규화 레이어 추가
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">lns</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">))</span>
        <span class="c1"># 총 3개 레이어 이므로 2개 추가
</span>        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">convs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">build_conv_model</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>

        <span class="c1"># post-message-passing
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">post_mp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">),</span> 
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">task</span> <span class="o">==</span> <span class="s">'node'</span> <span class="ow">or</span> <span class="bp">self</span><span class="p">.</span><span class="n">task</span> <span class="o">==</span> <span class="s">'graph'</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">RuntimeError</span><span class="p">(</span><span class="s">'Unknown task.'</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.25</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="k">def</span> <span class="nf">build_conv_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="c1"># refer to pytorch geometric nn module for different implementation of GNNs.
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">task</span> <span class="o">==</span> <span class="s">'node'</span><span class="p">:</span>
            <span class="c1"># 원래는 GCNConv를 이용하지만, 아래에서 구현해본 결과를 살펴보기 위해서 해볼 것!
</span>            <span class="c1">#return pyg_nn.GCNConv(input_dim, hidden_dim)
</span>            <span class="k">return</span> <span class="n">CustomConv</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pyg_nn</span><span class="p">.</span><span class="n">GINConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
                                  <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">batch</span>
        <span class="k">if</span> <span class="n">data</span><span class="p">.</span><span class="n">num_node_features</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">num_nodes</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 각 레이어 사이에는 ReLU와 오버피팅을 막기 위한 드롭아웃 추가
</span>        <span class="c1"># 그러나 마지막 레이어에는 추가하지 말아야함... 성능상 이유
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">convs</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">x</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lns</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># 그래프 분류의 경우에는 mean pool 진행해야함
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">task</span> <span class="o">==</span> <span class="s">'graph'</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="p">.</span><span class="n">global_mean_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        
        <span class="c1"># 분류를 위한 fc 통과
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post_mp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">emb</span><span class="p">,</span> <span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></table></code></div></div><h2 id="3단계-training">3단계: Training</h2><p>CNN 모델링과 동일하게 training은 흐름이 이루어집니다. <br /> 그런데 데이터 셋의 경우, 그래프 분류의 경우에는 len을 기준(80:20)으로 나누면 되지만, 노드 분류의 경우에는 그렇지 않기 때문에 <code class="language-plaintext highlighter-rouge">batch.train_mask</code>를 활용해서 진행하게 됩니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">task</span><span class="p">,</span> <span class="n">writer</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">task</span> <span class="o">==</span> <span class="s">'graph'</span><span class="p">:</span>
        <span class="c1"># 그래프 분류의 경우에는 단순하게 나누면 됨.
</span>        <span class="n">data_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">data_size</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)],</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">data_size</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">):],</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 노드 분류의 경우에는 batch.train_mask 활용할 것
</span>        <span class="n">test_loader</span> <span class="o">=</span> <span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># build model
</span>    <span class="c1"># GNNStack 대신 직접 구현한 GCN 모델 활용해볼 것
</span>    <span class="c1"># GCNCustomStack(이 경우에 결과가 비슷한가??)
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">GNNStack</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">num_node_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">32</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">)</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    
    <span class="c1"># train
</span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
            <span class="n">opt</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">embedding</span><span class="p">,</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="n">y</span>
            <span class="k">if</span> <span class="n">task</span> <span class="o">==</span> <span class="s">'node'</span><span class="p">:</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="n">batch</span><span class="p">.</span><span class="n">train_mask</span><span class="p">]</span>
                <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="n">batch</span><span class="p">.</span><span class="n">train_mask</span><span class="p">]</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">batch</span><span class="p">.</span><span class="n">num_graphs</span>
        <span class="n">total_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>
        <span class="n">writer</span><span class="p">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s">"loss"</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">test_acc</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
                <span class="n">epoch</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
            <span class="n">writer</span><span class="p">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s">"test accuracy"</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</pre></table></code></div></div><p>Test time, for the CiteSeer/Cora node classification task, there is only 1 graph. So we use masking to determine validation and test set.</p><p>For graph classification tasks, a subset of graphs is considered validation / test graph.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">is_validation</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>

    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">emb</span><span class="p">,</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span>

        <span class="k">if</span> <span class="n">model</span><span class="p">.</span><span class="n">task</span> <span class="o">==</span> <span class="s">'node'</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">val_mask</span> <span class="k">if</span> <span class="n">is_validation</span> <span class="k">else</span> <span class="n">data</span><span class="p">.</span><span class="n">test_mask</span>
            <span class="c1"># node classification: only evaluate on nodes in test set
</span>            <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
            
        <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">label</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">model</span><span class="p">.</span><span class="n">task</span> <span class="o">==</span> <span class="s">'graph'</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span> 
    <span class="k">else</span><span class="p">:</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">:</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">test_mask</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
</pre></table></code></div></div><h2 id="3단계-training-실행-및-tensorboard-출력">3단계: Training (실행 및 tensorboard 출력)</h2><p>이제 훈련과 출력을 진행합니다. <br /> 아래 명령어를 실행하면 <code class="language-plaintext highlighter-rouge">localhost:6006</code>에 텐서보드가 생성되고, 이를 통해서 log를 출력해볼 수 있습니다. <br /> <code class="language-plaintext highlighter-rouge">scalars</code> 탭에서 리프레시를 하면 로그가 출력됩니다.<br /></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">get_ipython</span><span class="p">().</span><span class="n">system_raw</span><span class="p">(</span>
    <span class="s">'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &amp;'</span>
    <span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"./log"</span><span class="p">)</span>
<span class="p">)</span>
</pre></table></code></div></div><p>텐서보드 좌측에서는 여러개 로그가 기록되었을 경우 골라서 살펴볼 수 있습니다.</p><p>이제 대표적인 그래프 데이터셋은 영화(IMDB-BINARY) 데이터셋을 분류해보겠습니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="s">"./log/"</span> <span class="o">+</span> <span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d-%H%M%S"</span><span class="p">))</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">TUDataset</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'/tmp/ENZYMES'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'ENZYMES'</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">shuffle</span><span class="p">()</span>
<span class="n">task</span> <span class="o">=</span> <span class="s">'graph'</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">task</span><span class="p">,</span> <span class="n">writer</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre>Epoch 0. Loss: 1.8338. Test accuracy: 0.1667
Epoch 10. Loss: 1.7953. Test accuracy: 0.1583
Epoch 20. Loss: 1.7908. Test accuracy: 0.2417
Epoch 30. Loss: 1.7723. Test accuracy: 0.1833
Epoch 40. Loss: 1.7629. Test accuracy: 0.1167
Epoch 50. Loss: 1.7631. Test accuracy: 0.1667
Epoch 60. Loss: 1.7592. Test accuracy: 0.1667
Epoch 70. Loss: 1.7387. Test accuracy: 0.2000
Epoch 80. Loss: 1.7008. Test accuracy: 0.2667
Epoch 90. Loss: 1.7049. Test accuracy: 0.2750
Epoch 100. Loss: 1.6777. Test accuracy: 0.3250
Epoch 110. Loss: 1.6876. Test accuracy: 0.2917
Epoch 120. Loss: 1.6645. Test accuracy: 0.3167
Epoch 130. Loss: 1.6491. Test accuracy: 0.2667
Epoch 140. Loss: 1.6404. Test accuracy: 0.3083
Epoch 150. Loss: 1.6450. Test accuracy: 0.3583
Epoch 160. Loss: 1.6248. Test accuracy: 0.3000
Epoch 170. Loss: 1.6396. Test accuracy: 0.2917
Epoch 180. Loss: 1.6282. Test accuracy: 0.3250
Epoch 190. Loss: 1.6063. Test accuracy: 0.3250
</pre></table></code></div></div><p>Citeseer citation network에 대해서는 노드 분류를 진행해보겠습니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="s">"./log/"</span> <span class="o">+</span> <span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d-%H%M%S"</span><span class="p">))</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">Planetoid</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'/tmp/cora'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'cora'</span><span class="p">)</span>
<span class="n">task</span> <span class="o">=</span> <span class="s">'node'</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">task</span><span class="p">,</span> <span class="n">writer</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre>Epoch 0. Loss: 2.0201. Test accuracy: 0.2460
Epoch 10. Loss: 0.3988. Test accuracy: 0.7940
Epoch 20. Loss: 0.0413. Test accuracy: 0.7750
Epoch 30. Loss: 0.0986. Test accuracy: 0.7600
Epoch 40. Loss: 0.0326. Test accuracy: 0.7530
Epoch 50. Loss: 0.0408. Test accuracy: 0.7480
Epoch 60. Loss: 0.0042. Test accuracy: 0.7580
Epoch 70. Loss: 0.0078. Test accuracy: 0.7590
Epoch 80. Loss: 0.0069. Test accuracy: 0.7490
Epoch 90. Loss: 0.0231. Test accuracy: 0.7530
Epoch 100. Loss: 0.0022. Test accuracy: 0.7620
Epoch 110. Loss: 0.0011. Test accuracy: 0.7750
Epoch 120. Loss: 0.0021. Test accuracy: 0.7760
Epoch 130. Loss: 0.0003. Test accuracy: 0.7590
Epoch 140. Loss: 0.0007. Test accuracy: 0.7710
Epoch 150. Loss: 0.0014. Test accuracy: 0.7840
Epoch 160. Loss: 0.0010. Test accuracy: 0.7770
Epoch 170. Loss: 0.0008. Test accuracy: 0.7690
Epoch 180. Loss: 0.0020. Test accuracy: 0.7530
Epoch 190. Loss: 0.0013. Test accuracy: 0.7350
</pre></table></code></div></div><h2 id="4단계-결과-시각화-노드-임베딩-2차원화">4단계: 결과 시각화 (노드 임베딩 2차원화)</h2><p>One great quality about graph neural networks is that, like other deep methods, their hidden layers provide low-dimensional representations of our data. In the case of node classification, we get a low-dimensional representation for each node in our graph. Let’s visualize the output of the last convolutional layer in our node classification GNN via TSNE, a method for plotting high-dimensional data. Nodes are colored according to their labels. We see that nodes with similar labels tend to be near each other in the embedding space, a good indication that our model has learned a useful representation.<br /> 분류한 결과값(모델을 통과시킨 값)이 군집이 잘 형성되었다는 것은 임베딩이 서로 가까웠다는 것을 의미하기 때문에 학습이 비교적 잘 이루어졌음을 의미합니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="n">color_list</span> <span class="o">=</span> <span class="p">[</span><span class="s">"red"</span><span class="p">,</span> <span class="s">"orange"</span><span class="p">,</span> <span class="s">"green"</span><span class="p">,</span> <span class="s">"blue"</span><span class="p">,</span> <span class="s">"purple"</span><span class="p">,</span> <span class="s">"brown"</span><span class="p">,</span> <span class="s">"yellow"</span><span class="p">]</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">embs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
    <span class="c1"># 모델을 통과시켜서 나온 결과를 볼 것
</span>    <span class="n">emb</span><span class="p">,</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">embs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
    <span class="n">colors</span> <span class="o">+=</span> <span class="p">[</span><span class="n">color_list</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">.</span><span class="n">y</span><span class="p">]</span>
<span class="n">embs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">embs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">TSNE</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embs</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">()))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>&lt;matplotlib.collections.PathCollection at 0x7f924857cb20&gt;
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-02-11-GNN_Pytorch/output_44_1.png" alt="png" /></p><h2 id="추가-learning-unsupervised-embeddings-with-graph-autoencoders-gae-활용">추가: Learning unsupervised embeddings with graph autoencoders (GAE 활용)</h2><p>Finally, GNNs fit nicely in the framework of other neural approaches, and can be used as part of autoencoder techniques, pretraining and multitask learning methods, etc. Here we explore the idea of neural network representations further by building a graph autoencoder which learns these representations in a completely unsupervised way. In contrast to the previous example, we do not make use of the given node labels when training this representation. Instead, we encode the nodes in our network in a low-dimensional space in such a way that the embeddings can be decoded into a reconstruction of the original network. We use graph convolutional layers in the encoder.</p><p>You can again use TensorBoardX here to visualize the training progress.</p><p>GCN 레이어를 이용해서 통과시킨 뒤에 Adj Matrix를 점곱으로 추정하는 것이 가능합니다.<br /> <code class="language-plaintext highlighter-rouge">Variational Graph Auto-Encoders</code> 논문 참고… (매우 쉬운 논문입니다.)<sup id="fnref:GAE" role="doc-noteref"><a href="#fn:GAE" class="footnote">3</a></sup> <br /> 이를 통해서 unsupservised learning이 가능합니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="p">.</span><span class="n">GCNConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="p">.</span><span class="n">GCNConv</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">recon_loss</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    
    <span class="n">writer</span><span class="p">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s">"loss"</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">epoch</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">pos_edge_index</span><span class="p">,</span> <span class="n">neg_edge_index</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">test</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">pos_edge_index</span><span class="p">,</span> <span class="n">neg_edge_index</span><span class="p">)</span>

<span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="s">"./log/"</span> <span class="o">+</span> <span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d-%H%M%S"</span><span class="p">))</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">Planetoid</span><span class="p">(</span><span class="s">"/tmp/citeseer"</span><span class="p">,</span> <span class="s">"Citeseer"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s">'random'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">T</span><span class="p">.</span><span class="n">NormalizeFeatures</span><span class="p">())</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">channels</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">dev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'CUDA availability:'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">())</span>

<span class="c1"># encoder: written by us; decoder: default (inner product)
# 점곱을 계산하는 GAE를 이용해서 앞에서 계산한 GCN을 통과시킨 것과 비교해봅니다.
</span><span class="n">model</span> <span class="o">=</span> <span class="n">pyg_nn</span><span class="p">.</span><span class="n">GAE</span><span class="p">(</span><span class="n">Encoder</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">num_features</span><span class="p">,</span> <span class="n">channels</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span>
<span class="n">data</span><span class="p">.</span><span class="n">train_mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">val_mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">test_mask</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pyg_utils</span><span class="p">.</span><span class="n">train_test_split_edges</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="c1"># 새로운 util인 https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/utils/train_test_split_edges.html
# train_test_split_edges 이용...
# 원래 (현재 작동하지 않는 코드)
# data = model.split_edges(data)
</span><span class="n">x</span><span class="p">,</span> <span class="n">train_pos_edge_index</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">),</span> <span class="n">data</span><span class="p">.</span><span class="n">train_pos_edge_index</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">201</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="n">auc</span><span class="p">,</span> <span class="n">ap</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">test_pos_edge_index</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">test_neg_edge_index</span><span class="p">)</span>
    <span class="n">writer</span><span class="p">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s">"AUC"</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="n">writer</span><span class="p">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s">"AP"</span><span class="p">,</span> <span class="n">ap</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">ap</span><span class="p">))</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre>CUDA availability: False
Epoch: 010, AUC: 0.6806, AP: 0.7383
Epoch: 020, AUC: 0.7478, AP: 0.7696
Epoch: 030, AUC: 0.7557, AP: 0.7726
Epoch: 040, AUC: 0.7538, AP: 0.7698
Epoch: 050, AUC: 0.7617, AP: 0.7778
Epoch: 060, AUC: 0.7841, AP: 0.7987
Epoch: 070, AUC: 0.8270, AP: 0.8315
Epoch: 080, AUC: 0.8264, AP: 0.8339
Epoch: 090, AUC: 0.8274, AP: 0.8358
Epoch: 100, AUC: 0.8304, AP: 0.8370
Epoch: 110, AUC: 0.8351, AP: 0.8428
Epoch: 120, AUC: 0.8470, AP: 0.8565
Epoch: 130, AUC: 0.8493, AP: 0.8600
Epoch: 140, AUC: 0.8470, AP: 0.8580
Epoch: 150, AUC: 0.8487, AP: 0.8595
Epoch: 160, AUC: 0.8450, AP: 0.8556
Epoch: 170, AUC: 0.8434, AP: 0.8548
Epoch: 180, AUC: 0.8458, AP: 0.8559
Epoch: 190, AUC: 0.8461, AP: 0.8557
Epoch: 200, AUC: 0.8529, AP: 0.8609
</pre></table></code></div></div><p>보면 논문과 유사한 수준의 결과가 나온 것을 알 수 있습니다(경우에 따라 4-5프로 가량 낮게 나오기도 하는데 더 돌려주면 나옵니다)<br /> Finally, we plot our embeddings (the output of the encoder) with TSNE. We color each node embedding according to its label – but note that we did not use any label information when training our encoder. Nodes with the same label are nevetheless close together in the embedding space. The model has learned the community structure without supervision!</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_pos_edge_index</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">color_list</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>

<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">TSNE</span><span class="p">().</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">()))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-02-11-GNN_Pytorch/output_49_0.png" alt="png" /></p><h2 id="references">References</h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:CS224w" role="doc-endnote"><p>http://web.stanford.edu/class/cs224w/ <a href="#fnref:CS224w" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:PyG_Doc" role="doc-endnote"><p>https://pytorch-geometric.readthedocs.io/en/latest/index.html <a href="#fnref:PyG_Doc" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:GAE" role="doc-endnote"><p>https://arxiv.org/abs/1611.07308 <a href="#fnref:GAE" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/deep-learning/'>Deep Learning</a>, <a href='/categories/gnn/'>GNN</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >Deep Learning</a> <a href="/tags/graph/" class="post-tag no-text-decoration" >Graph</a> <a href="/tags/gnn/" class="post-tag no-text-decoration" >GNN</a> <a href="/tags/gcn/" class="post-tag no-text-decoration" >GCN</a> <a href="/tags/ginconv/" class="post-tag no-text-decoration" >GINConv</a> <a href="/tags/gae/" class="post-tag no-text-decoration" >GAE</a> <a href="/tags/pytorch/" class="post-tag no-text-decoration" >Pytorch</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=GNN PyTorch Code Basic (GCN, GINConv, GAE) - Jaeyong&url=https://jaeyong-song.github.io/posts/GNN_Pytorch_Basic/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=GNN PyTorch Code Basic (GCN, GINConv, GAE) - Jaeyong&u=https://jaeyong-song.github.io/posts/GNN_Pytorch_Basic/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=GNN PyTorch Code Basic (GCN, GINConv, GAE) - Jaeyong&url=https://jaeyong-song.github.io/posts/GNN_Pytorch_Basic/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/GNN_Basic/">Graph Neural Networks(GNN) Basic (GCN, GraphSAGE, GAT)</a><li><a href="/posts/GNN_Pytorch_Basic/">GNN PyTorch Code Basic (GCN, GINConv, GAE)</a><li><a href="/posts/ResNet_Pytorch/">GNN PinSAGE Modeling with PyTorch (MovieLens 1M)</a><li><a href="/posts/IGMC_RecSys_GNN/">RecSys GNN Modeling (Inductive Matrix Completion Based on Graph Neural Networks, IGMC) (논문리뷰 및 코드분석, ICLR 2020)</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/gcn/">GCN</a> <a class="post-tag" href="/tags/gnn/">GNN</a> <a class="post-tag" href="/tags/graph/">Graph</a> <a class="post-tag" href="/tags/pytorch/">Pytorch</a> <a class="post-tag" href="/tags/dgl/">DGL</a> <a class="post-tag" href="/tags/gae/">GAE</a> <a class="post-tag" href="/tags/gat/">GAT</a> <a class="post-tag" href="/tags/ginconv/">GINConv</a> <a class="post-tag" href="/tags/graphsage/">GraphSAGE</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/GNN_Basic/"><div class="card-body"> <span class="timeago small" > Feb 11 <i class="unloaded">2021-02-11T20:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Graph Neural Networks(GNN) Basic (GCN, GraphSAGE, GAT)</h3><div class="text-muted small"><p> 데이터사이언스랩 2021년 2월 18일 (목) 심화세션 “GNN 이론”을 위해서 작성한 게시물입니다. GNN 관련 여러 게시물1과 강의2 를 참고하여 만들었습니다. 이번 게시물에서는 GCN, GraphSAGE, GAT을 위주로 살펴보도록 하겠습니다. ...</p></div></div></a></div><div class="card"> <a href="/posts/ResNet_Pytorch/"><div class="card-body"> <span class="timeago small" > Feb 16 <i class="unloaded">2021-02-16T18:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>GNN PinSAGE Modeling with PyTorch (MovieLens 1M)</h3><div class="text-muted small"><p> 데이터사이언스랩 2021년 2월 23일 (화) 심화세션 “GNN 모델링 실습”을 위해서 작성한 게시물입니다. GNN 관련 여러 게시물과 강의를 참고하여 만들었습니다.1 2 이번 게시물에서는 pytorch_geometric과 dgl을 이용하여 PinSAGE 모델을 사용해보고 이를 바탕으로 추천시스템을 구현하는 실습을 진행하겠습니다. 아직 작성중… ...</p></div></div></a></div><div class="card"> <a href="/posts/IGMC_RecSys_GNN/"><div class="card-body"> <span class="timeago small" > Feb 16 <i class="unloaded">2021-02-16T20:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RecSys GNN Modeling (Inductive Matrix Completion Based on Graph Neural Networks, IGMC) (논문리뷰 및 코드분석, ICLR 2020)</h3><div class="text-muted small"><p> 데이터사이언스랩 심화스터디 추천시스템2에서 GNN Modeling 관련하여 IGMC 논문을 리뷰한 내용입니다…1 2 정리중 References https://arxiv.org/abs/1904.12058 &#8617; https://github.com/muhanzhang/IGMC &#8617;...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/GNN_Basic/" class="btn btn-outline-primary" prompt="Older"><p>Graph Neural Networks(GNN) Basic (GCN, GraphSAGE, GAT)</p></a> <a href="/posts/ResNet_Pytorch/" class="btn btn-outline-primary" prompt="Newer"><p>GNN PinSAGE Modeling with PyTorch (MovieLens 1M)</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://github.com/jaeyong-song">Jaeyong Song</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/gcn/">GCN</a> <a class="post-tag" href="/tags/gnn/">GNN</a> <a class="post-tag" href="/tags/graph/">Graph</a> <a class="post-tag" href="/tags/pytorch/">Pytorch</a> <a class="post-tag" href="/tags/dgl/">DGL</a> <a class="post-tag" href="/tags/gae/">GAE</a> <a class="post-tag" href="/tags/gat/">GAT</a> <a class="post-tag" href="/tags/ginconv/">GINConv</a> <a class="post-tag" href="/tags/graphsage/">GraphSAGE</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { let initTheme = "default"; if ($("html[mode=dark]").length > 0 || ($("html[mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://jaeyong-song.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
